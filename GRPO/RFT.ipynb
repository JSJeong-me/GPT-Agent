{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT-Agent/blob/main/GRPO/RFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Calling with RFT\n",
        "\n",
        "This notebook will show you how to train an adapter with RFT on a function calling use case.\n",
        "\n",
        "Function calling (or tool calling) is a type of prompting/inference in which the model has is given the ability to use a set of tools to generate a response. For example, you might provide the model with a function that multiplies two numbers, and provide it with parameters to pass to the multiply function. When prompted, the model will use the multiply function to generate the product of two numbers.\n",
        "\n",
        "Here, we train a model to select the correct tools and parameters to pass those tools given a request and a set of available tools."
      ],
      "metadata": {
        "id": "9ir4wQ2Q2JtA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1yBeTvBfRZv"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall numpy -y --quiet\n",
        "# !pip install predibase --quiet\n",
        "# # !pip install datasets --quiet\n",
        "# ! pip install --upgrade datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall numpy -y --quiet\n",
        "# !pip install numpy==1.24.0 --quiet # Install a known compatible version of numpy\n",
        "# !pip install predibase --quiet\n",
        "# ! pip install --upgrade datasets huggingface_hub"
      ],
      "metadata": {
        "id": "CvUBMGBwiQDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y --quiet\n",
        "!pip install numpy==1.24.0 --quiet # Install a known compatible version of numpy\n",
        "# Uninstall and reinstall predibase and datasets to ensure compatibility with the installed numpy version\n",
        "!pip uninstall predibase -y --quiet\n",
        "!pip uninstall datasets -y --quiet\n",
        "!pip install predibase --quiet\n",
        "! pip install --upgrade datasets huggingface_hub"
      ],
      "metadata": {
        "id": "CQVQfDs1ilBN",
        "outputId": "93bcabfd-cc8b-413f-9c28-a6254093f2b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 2.0.7 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: typer 0.15.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datasets\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.24.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Installing collected packages: datasets\n",
            "Successfully installed datasets-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Your Data\n",
        "\n",
        "For RFT, you need, at minimum, a text dataset with a `prompt` field. Your dataset can optionally have other columns as well. When provided, these columns are accessible within your defined reward functions if you need to access these fields and their values.\n",
        "\n",
        "For this function calling use case, you can get the dataset from Predibase on HuggingFace Hub.\n",
        "\n",
        "[How to prepare your data for GRPO](https://docs.predibase.com/user-guide/fine-tuning/grpo#prepare-your-dataset)"
      ],
      "metadata": {
        "id": "jPnohwFN1rgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"predibase/glaive_function_calling\")\n",
        "train_df = pd.DataFrame(dataset[\"train\"])\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "gbGjEUMn1pVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SAMPLE PROMPT\")\n",
        "print(train_df['prompt'][2])\n",
        "print(\"\\n\\nSAMPLE TOOL CALL\")\n",
        "print(train_df['tool_call'][2])"
      ],
      "metadata": {
        "id": "OA10ROiR8Hna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_json('./glaive_function_calling.jsonl', lines=True, orient='records')"
      ],
      "metadata": {
        "id": "BdeOT3ruG44_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Your Reward Functions\n",
        "\n",
        "Reward functions will be used to score your model's generations during training.\n",
        "\n",
        "All reward functions must follow this function signature:\n",
        "\n",
        "```\n",
        "def reward_fn(prompt: str, completion: str, example: dict[str, str]) -> float\n",
        "```\n",
        "\n",
        "The prompt is a prompt from your dataset. The completion is one of N of the model's generated outputs for the given prompt. The example is the original data sample from your dataset represented as a dictionary. If you define more than one reward function, you should make sure you give each reward function a unique function name.\n",
        "\n",
        "[Reward Functions Documentation](https://docs.predibase.com/user-guide/fine-tuning/grpo#defining-reward-functions)"
      ],
      "metadata": {
        "id": "pQOLCSpPvgby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_tool_use_reward_func(prompt: str, completion: str, example: dict):\n",
        "    \"\"\"Check that the correct tools are being called, max score is 1.0.\"\"\"\n",
        "    import re\n",
        "    import ast\n",
        "    import json\n",
        "\n",
        "    rscale = 1.0\n",
        "    reward = 0.0\n",
        "\n",
        "    true_tool_call = ast.literal_eval(example['tool_call'])\n",
        "\n",
        "    try:\n",
        "        pred_tool_call = re.search(r'<tool>(.*?)</tool>', completion, re.DOTALL).group(1).strip()\n",
        "        pred_tool_call = ast.literal_eval(pred_tool_call)\n",
        "        if pred_tool_call != '':\n",
        "            assert isinstance(pred_tool_call, dict)\n",
        "            assert 'name' in pred_tool_call\n",
        "            assert 'arguments' in pred_tool_call\n",
        "    except Exception as e:\n",
        "        print(f'Error parsing tool call: {e} for {completion}')\n",
        "        return reward * rscale\n",
        "\n",
        "    if pred_tool_call == '':\n",
        "        pred_tool_call = None\n",
        "\n",
        "    # If both don't use a tool then we give full credit\n",
        "    if true_tool_call is None and pred_tool_call is None:\n",
        "        reward += 1.0\n",
        "        print(f'(CORRECT - NO TOOL). global tool use reward: {reward}')\n",
        "        return reward * rscale\n",
        "\n",
        "    # If both use a tool\n",
        "    if true_tool_call is not None and pred_tool_call is not None:\n",
        "        # For using a tool when true_tool_call is not None\n",
        "        reward += 0.1\n",
        "\n",
        "        # Why is this necessary and only happens for true_tool_call? Some issue with data upload and reload vs in completion, ast literal eval is enough.\n",
        "        if isinstance(true_tool_call['arguments'], str):\n",
        "            true_tool_call['arguments'] = json.loads(true_tool_call['arguments'])\n",
        "\n",
        "        # Name match\n",
        "        if true_tool_call['name'] != pred_tool_call['name']:\n",
        "            print(f'(PARTIAL - NAME) {true_tool_call=}, {pred_tool_call=} did not match. global tool use reward: {reward}')\n",
        "            return reward * rscale\n",
        "        reward += 0.4\n",
        "\n",
        "        # Arguments match\n",
        "        if true_tool_call['arguments'] != pred_tool_call['arguments']:\n",
        "            print(f'(PARTIAL - ARGUMENTS) {true_tool_call=}, {pred_tool_call=} did not match. global tool use reward: {reward}')\n",
        "            return reward * rscale\n",
        "\n",
        "        reward += 0.5\n",
        "        print(f'(CORRECT) {true_tool_call=}, {pred_tool_call=} global tool use reward: {reward}')\n",
        "        return reward * rscale\n",
        "    else:\n",
        "        print(f'(INCORRECT - TYPE) {true_tool_call=}, {pred_tool_call=} did not match. global tool use reward: {reward}')\n",
        "        return reward * rscale"
      ],
      "metadata": {
        "id": "ioTZlUGnpYvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_format_reward_func(prompt: str, completion: str, example: dict):\n",
        "    \"\"\"Check that the generated text is in the requested format, max score is 1.0.\"\"\"\n",
        "    import re\n",
        "    import ast\n",
        "\n",
        "    reward = 0.0\n",
        "    rscale = 0.5\n",
        "\n",
        "    completion = f'<think>{completion}'\n",
        "\n",
        "    # Find <think> and </think> tags\n",
        "    think_start = completion.find('<think>')\n",
        "    think_end = completion.find('</think>')\n",
        "\n",
        "    # Find <functioncall> or <no_functioncall> tags\n",
        "    tool_start = completion.find('<tool>')\n",
        "    tool_end = completion.find('</tool>')\n",
        "\n",
        "    if think_start == -1 or think_end == -1:\n",
        "        print(f'(PARTIAL - FORMAT) missing think or tool tags. format reward: {reward}')\n",
        "        return reward * rscale\n",
        "\n",
        "    reward += 0.1\n",
        "\n",
        "    if not (think_start < think_end < tool_start < tool_end):\n",
        "        print(f'(PARTIAL - FORMAT) tags present but not in the correct order. format reward: {reward}')\n",
        "        return reward * rscale\n",
        "\n",
        "    reward += 0.1\n",
        "\n",
        "    # Check if there are any stray tags\n",
        "    think_tags = re.findall(r'</?think>', completion)\n",
        "    tool_tags = re.findall(r'</?tool>', completion)\n",
        "\n",
        "    if len(think_tags) != 2 or len(tool_tags) != 2:\n",
        "        print(f'(PARTIAL - FORMAT) found stray think or tool tags. format reward: {reward}')\n",
        "        return reward * rscale\n",
        "\n",
        "    reward += 0.2\n",
        "\n",
        "    # Check if tool call syntax is valid.\n",
        "    try:\n",
        "        pred_tool_call = re.search(r'<tool>(.*?)</tool>', completion, re.DOTALL).group(1).strip()\n",
        "        pred_tool_call = ast.literal_eval(pred_tool_call)\n",
        "        if pred_tool_call != '':\n",
        "            assert isinstance(pred_tool_call, dict)\n",
        "            assert 'name' in pred_tool_call\n",
        "            assert 'arguments' in pred_tool_call\n",
        "    except Exception as e:\n",
        "        print(f'(PARTIAL - FORMAT) could not parse result. Exception: {e} for {pred_tool_call=}. format reward: {reward}')\n",
        "        return reward * rscale\n",
        "\n",
        "    reward += 0.6\n",
        "\n",
        "    return reward * rscale"
      ],
      "metadata": {
        "id": "WDshp_a8p2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_length_reward_func(prompt: str, completion: str, example: dict):\n",
        "    \"\"\"Set a hard limit on the completion length, max score is 1.0\"\"\"\n",
        "    import math\n",
        "    norm_length_char = 2000 # 500 tokens * 4 characters per token\n",
        "    num_chars = len(completion)\n",
        "    return 1.0 if num_chars <= norm_length_char else 0.0"
      ],
      "metadata": {
        "id": "2oVogjMXqQ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Predibase Client and Upload Your Data"
      ],
      "metadata": {
        "id": "3cnZSrqOv5dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from predibase import Predibase\n",
        "from google.colab import userdata\n",
        "\n",
        "pb = Predibase(api_token=userdata.get(\"PREDIBASE_API_TOKEN\"))"
      ],
      "metadata": {
        "id": "HwvWhTw7hVMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  dataset = pb.datasets.from_file(\"./glaive_function_calling.jsonl\", name=\"glaive_function_calling\")\n",
        "except:\n",
        "  dataset = pb.datasets.get(\"glaive_function_calling\")"
      ],
      "metadata": {
        "id": "KblY1-MlhleD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with RFT!\n",
        "\n",
        "Set up an adapter repo and start training your models! To train with RFT, pass a `GRPOConfig` with the name of the base model you would like to fine-tune and your rewards functions. You can also optionally adjust the parameters that control sampling your model--like temperature and the max tokens to generate.\n",
        "\n",
        "[Supported Base Models List](https://docs.predibase.com/user-guide/fine-tuning/finetuning-models)\n",
        "\n",
        "[`GRPOConfig` Documentation](https://docs.predibase.com/sdk-guide/SDKv2/ConfigClasses/FineTuningConfig#grpoconfig)\n",
        "\n",
        "[`RewardFunctionsConfig` Documentation](https://docs.predibase.com/sdk-guide/SDKv2/ConfigClasses/RewardFunctionsConfig)\n",
        "\n",
        "[`SamplingParamsConfig` Documentation](https://docs.predibase.com/sdk-guide/SDKv2/ConfigClasses/SamplingParamsConfig)\n"
      ],
      "metadata": {
        "id": "EAnsnO1BwHY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo = pb.repos.create(name=\"function-calling-rft\", description=\"Train a function calling model with RFT!\", exists_ok=True)"
      ],
      "metadata": {
        "id": "_f39tDgEo7tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from predibase import GRPOConfig, RewardFunctionsConfig, SamplingParamsConfig\n",
        "\n",
        "adapter = pb.finetuning.jobs.create(\n",
        "    config=GRPOConfig(\n",
        "        base_model=\"qwen2-5-7b-instruct\",\n",
        "        num_generations=16,\n",
        "        sampling_params=SamplingParamsConfig(\n",
        "            max_tokens=1024\n",
        "        ),\n",
        "        reward_fns=RewardFunctionsConfig(\n",
        "            functions={\n",
        "                \"correctness\": global_tool_use_reward_func,\n",
        "                \"format\": global_format_reward_func,\n",
        "                \"length\": global_length_reward_func\n",
        "            },\n",
        "        ),\n",
        "    ),\n",
        "    dataset=dataset,\n",
        "    repo=repo,\n",
        "    description=\"Function calling model\",\n",
        ")"
      ],
      "metadata": {
        "id": "eebzm4bUpVe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update the Reward Functions\n",
        "\n",
        "During training, you may notice that the model is not performing well on a particular reward. Maybe the reward function is too restrictive, or there is a bug in its logic. If this is the case, you can update your reward functions in a training job without interrupting training. You just need to submit an updated reward function to the running job with a name matching an existing reward function.\n",
        "\n",
        "[How to Update a Rewar Function](https://docs.predibase.com/user-guide/fine-tuning/grpo#how-do-i-update-my-reward-functions)\n"
      ],
      "metadata": {
        "id": "D0hIMY5Ayukq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_length_reward_func_v2(prompt: str, completion: str, example: dict):\n",
        "    \"\"\"Minimize completion length, max score is 1.0\"\"\"\n",
        "    import math\n",
        "    rscale = 0.5\n",
        "    norm_length_char = 2000 # 500 tokens * 4 characters per token\n",
        "    num_chars = len(completion)\n",
        "\n",
        "    def tanh(x):\n",
        "        return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
        "\n",
        "    reward =  1 - tanh(num_chars / norm_length_char)\n",
        "\n",
        "    return reward * rscale"
      ],
      "metadata": {
        "id": "sO_sWYIVzoa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_path = \"function-calling-rft/1\"\n",
        "\n",
        "# Get the original config and update it with the new reward function\n",
        "cfg = pb.adapters.get_config(adapter_path)\n",
        "cfg.reward_fns[\"length\"] = global_length_reward_func_v2\n",
        "pb.adapters.update_config(adapter_path, cfg)"
      ],
      "metadata": {
        "id": "Yafk1FJG0Unh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wg_OpUTfT4v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}