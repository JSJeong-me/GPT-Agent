{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzMgtPx3tZPzntpY3CwtYs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT-Agent/blob/main/Memory/LangMem02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHZnFkljAUqj"
      },
      "outputs": [],
      "source": [
        "!pip install -U langmem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 사용자에게 안전하게 API 키 입력 받기\n",
        "anthropic_api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "\n",
        "# 환경 변수로 설정\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key"
      ],
      "metadata": {
        "id": "k6_rXQVQCv1F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key  = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "sp3gCBbnEpTa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.utils.config import get_store\n",
        "from langmem import (\n",
        "    # Lets agent create, update, and delete memories\n",
        "    create_manage_memory_tool,\n",
        ")\n",
        "\n",
        "\n",
        "def prompt(state):\n",
        "    \"\"\"Prepare the messages for the LLM.\"\"\"\n",
        "    # Get store from configured contextvar;\n",
        "    store = get_store() # Same as that provided to `create_react_agent`\n",
        "    memories = store.search(\n",
        "        # Search within the same namespace as the one\n",
        "        # we've configured for the agent\n",
        "        (\"memories\",),\n",
        "        query=state[\"messages\"][-1].content,\n",
        "    )\n",
        "    system_msg = f\"\"\"You are a helpful assistant.\n",
        "\n",
        "## Memories\n",
        "<memories>\n",
        "{memories}\n",
        "</memories>\n",
        "\"\"\"\n",
        "    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n",
        "\n",
        "\n",
        "store = InMemoryStore(\n",
        "    index={ # Store extracted memories\n",
        "        \"dims\": 1536,\n",
        "        \"embed\": \"openai:text-embedding-3-small\",\n",
        "    }\n",
        ")\n",
        "checkpointer = MemorySaver() # Checkpoint graph state\n",
        "\n",
        "agent = create_react_agent(\n",
        "    \"anthropic:claude-3-5-sonnet-latest\",\n",
        "    prompt=prompt,\n",
        "    tools=[ # Add memory tools\n",
        "        # The agent can call \"manage_memory\" to\n",
        "        # create, update, and delete memories by ID\n",
        "        # Namespaces add scope to memories. To\n",
        "        # scope memories per-user, do (\"memories\", \"{user_id}\"):\n",
        "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
        "    ],\n",
        "    # Our memories will be stored in this provided BaseStore instance\n",
        "    store=store,\n",
        "    # And the graph \"state\" will be checkpointed after each node\n",
        "    # completes executing for tracking the chat history and durable execution\n",
        "    checkpointer=checkpointer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReXdq1U8GHhp",
        "outputId": "6e03d23b-ae55-441f-fe68-196be9b912f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langgraph/store/base/embed.py:95: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
            "  return init_embeddings(embed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"thread-a\"}}\n",
        "\n",
        "# Use the agent. The agent hasn't saved any memories,\n",
        "# so it doesn't know about us\n",
        "response = agent.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Know which display mode I prefer?\"}\n",
        "        ]\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "print(response[\"messages\"][-1].content)\n",
        "# Output: \"I don't seem to have any stored memories about your display mode preferences...\"\n",
        "\n",
        "agent.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"dark. Remember that.\"}\n",
        "        ]\n",
        "    },\n",
        "    # We will continue the conversation (thread-a) by using the config with\n",
        "    # the same thread_id\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# New thread = new conversation!\n",
        "new_config = {\"configurable\": {\"thread_id\": \"thread-b\"}}\n",
        "# The agent will only be able to recall\n",
        "# whatever it explicitly saved using the manage_memories tool\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hey there. Do you remember me? What are my preferences?\"}]},\n",
        "    config=new_config,\n",
        ")\n",
        "print(response[\"messages\"][-1].content)\n",
        "# Output: \"Based on my memory search, I can see that you've previously indicated a preference for dark display mode...\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uvfm2r2GpTC",
        "outputId": "b33fd093-c2c5-485b-a460-30e46aa89553"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let me check if I have any stored memories about your display mode preferences.\n",
            "\n",
            "I don't see any memories stored about your display mode preferences. I don't know your preferred display mode yet. Would you like to tell me your preference so I can remember it for future conversations?\n",
            "Yes, based on the memories I have access to, I know that you prefer dark display mode. This preference was recorded in my memory system.\n",
            "\n",
            "Would you like me to help you with anything specific based on this preference, or would you like to share any other preferences you'd like me to remember?\n"
          ]
        }
      ]
    }
  ]
}